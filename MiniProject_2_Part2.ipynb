{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d0e9b954427945b6b6e7113b6da8ed4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77d623e5263d48adb53afe763855f354",
              "IPY_MODEL_2412b9bac6d0483f983fd990192e713f",
              "IPY_MODEL_e9a17fa718904828a0dfaea70e53df02"
            ],
            "layout": "IPY_MODEL_58b1d638482844b0a8e56100977b34b3"
          }
        },
        "77d623e5263d48adb53afe763855f354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f78ba750e4e44d699be540feb9c884c5",
            "placeholder": "​",
            "style": "IPY_MODEL_c7777544c93c47589275e232aaac8398",
            "value": "Pandas Apply:  41%"
          }
        },
        "2412b9bac6d0483f983fd990192e713f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a976213270304e79a6b9cd68d76a4f0d",
            "max": 22281,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11a4c97f35664a2cbe38105602e190f8",
            "value": 9025
          }
        },
        "e9a17fa718904828a0dfaea70e53df02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c1d294bd6bf4b76a7cf5e2183b5cac7",
            "placeholder": "​",
            "style": "IPY_MODEL_349ac179825a419c88166bee84053bc9",
            "value": " 9025/22281 [1:24:20&lt;1:59:11,  1.85it/s]"
          }
        },
        "58b1d638482844b0a8e56100977b34b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f78ba750e4e44d699be540feb9c884c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7777544c93c47589275e232aaac8398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a976213270304e79a6b9cd68d76a4f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11a4c97f35664a2cbe38105602e190f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c1d294bd6bf4b76a7cf5e2183b5cac7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349ac179825a419c88166bee84053bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsfWwT9a1bxJ",
        "outputId": "de3aa7a5-611e-49e1-ffaa-812fa3da0717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Importing all necessary files\n",
        "from lxml import etree, objectify\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import pandas as pd \n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter\n",
        "import swifter"
      ],
      "metadata": {
        "id": "LuXyIhug_Rwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get sense from original dictionary: \n",
        "def getSenses(word, pos):\n",
        "    #global Tree\n",
        "    item = Tree.xpath(\"//lexelt[@item='%s.%s']\" % (word, pos))    \n",
        "    senses = []\n",
        "    if len(item) >= 1:\n",
        "        for sense in item[0].getchildren():\n",
        "            senses.append(dict(zip(sense.keys(), sense.values())))\n",
        "    return senses"
      ],
      "metadata": {
        "id": "95lnrGRr1jtj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions to clean all the datasets\n",
        "#rename columns and ignore the index column\n",
        "def rename_columns(dataset):\n",
        "    dataset_new = dataset.rename(columns = {0:\"Target_Word\", 1:\"Sense_ID\", 2:\"Sentence\"})\n",
        "    dataset_new = dataset_new.reset_index(drop=True)\n",
        "    return dataset_new"
      ],
      "metadata": {
        "id": "NzjoP0bg3qau"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert sentence column to lower case, remove digits and punctuations\n",
        "def lowercase_cleaned_data(dataset, colname):\n",
        "    stop = stopwords.words('english')\n",
        "    string.punctuation = string.punctuation.replace('%', '')\n",
        "    dataset[\"lowercase_cleaned\"] = dataset[colname].apply(lambda words: ' '.join(word.lower().translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for word in words.split()))\n",
        "    dataset[\"lowercase_cleaned\"] = dataset[\"lowercase_cleaned\"].str.replace('\\d+', '')\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "QS_2COMe3sZ1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieving pos for the words and lemmatisation\n",
        "def retreive_pos_wordnet(sentence):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    sentence = ' '.join(word.lower().translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for word in sentence.split())\n",
        "    list_words = sentence.split()\n",
        "    final_list = []\n",
        "    for i in range (len(list_words)):\n",
        "        tag = nltk.pos_tag(list_words)[i][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "        final_tag = tag_dict.get(tag, wordnet.NOUN)\n",
        "        lemmatized_word = lemmatizer.lemmatize(list_words[i],final_tag)\n",
        "        final_list.append([list_words[i],final_tag,lemmatized_word])\n",
        "    return final_list"
      ],
      "metadata": {
        "id": "eLZiKPiP341t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove stop words and words with length < 3\n",
        "def remove_stop_words_from_pos(pos_input_list):\n",
        "    return_list = []\n",
        "    stop = stopwords.words('english')\n",
        "    for pos in pos_input_list:\n",
        "        if (pos[2] not in stop and (len(pos[2])>2 or pos[2]==\"%%\")):\n",
        "            return_list.append(pos)\n",
        "    return return_list"
      ],
      "metadata": {
        "id": "050Kqh074Erp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to clean dictionary\n",
        "def lemmatize_sentences(sentence):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    sentence = ' '.join(word.lower().translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for word in sentence.split())\n",
        "    list_words = sentence.split()\n",
        "    lemmatize_words = ''\n",
        "    for i in range (len(list_words)):\n",
        "        tag = nltk.pos_tag(list_words)[i][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "        final_tag = tag_dict.get(tag, wordnet.NOUN)\n",
        "        lemmatize_words += \" \" + lemmatizer.lemmatize(list_words[i],final_tag)   \n",
        "    return lemmatize_words.strip()"
      ],
      "metadata": {
        "id": "2GE871Pi4L5l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getLemmaExamplesFromSenseDict(word_pos, sense):\n",
        "    word_pos = word_pos.strip()\n",
        "    lemmaSenseKey = word_pos+ \"_\"+sense.get('id')\n",
        "    sense_examples = \"\"\n",
        "    if (lemmaSenseKey in SenseLemmaDictionary):\n",
        "        sense_examples = SenseLemmaDictionary.get(lemmaSenseKey)\n",
        "    else:\n",
        "        sense_examples = (\n",
        "            lemmatize_sentences(sense.get('gloss').lower())\n",
        "            + \" | \"\n",
        "            + ('.'.join(lemmatize_sentences(sentence.lower()) for sentence in sense.get('examples').split(\".\")))\n",
        "        )\n",
        "        SenseLemmaDictionary[lemmaSenseKey] = sense_examples\n",
        "    return sense_examples"
      ],
      "metadata": {
        "id": "5ez_2Kuj4px3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1 : Simple lesk Algorithm\n",
        "def calculate_sense_model_one(target_word, pos_data):\n",
        "    #print(target_word)\n",
        "    target_data = target_word.split(\".\")\n",
        "    senses = getSenses(target_data[0].strip(), target_data[1].strip())\n",
        "    score_map = {}\n",
        "    pos_sentence = []\n",
        "    for pos_word in pos_data:\n",
        "        pos_sentence.append(pos_word[2])\n",
        "    \n",
        "    for sense in senses:\n",
        "        sense_score = 0\n",
        "        sense_examples = getLemmaExamplesFromSenseDict(target_word, sense)\n",
        "        sense_example_words = sense_examples.split()\n",
        "        common = set(sense_example_words).intersection( set(pos_sentence) )\n",
        "        score_map[sense.get('id')] = len(common)\n",
        "    \n",
        "    key_max = max(score_map, key=score_map.get)\n",
        "    return key_max"
      ],
      "metadata": {
        "id": "OQAP5lO36p8c"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracies of all models\n",
        "def calculate_accuracy(dataframe, column_name):\n",
        "    accuracy_number = 0\n",
        "    i=0\n",
        "    for index, row in dataframe.iterrows():\n",
        "        if(int(row['Sense_ID'])==int(row[column_name])):\n",
        "            accuracy_number += 1\n",
        "        i += 1\n",
        "    return ((accuracy_number/i)*100)\n",
        "\n",
        "# Exporting to CSV\n",
        "def exportToCSV(input_data_frame, csv_path):\n",
        "    tmp_df = input_data_frame.drop(['Sentence', 'lowercase_cleaned', 'pos_data'], axis=1)\n",
        "    tmp_df.to_csv(csv_path, index = False)"
      ],
      "metadata": {
        "id": "25cKsW0q7lb4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2 : Orginal lesk Algorithm\n",
        "def limitizeContextMapModelTwo(context_sense):\n",
        "    dictionary_examples = \"\"\n",
        "    for context_data in context_sense:\n",
        "        for sense_data in context_data[2]:\n",
        "            #dictionary_examples += lemmatize_sentences(sense_data.get('gloss').lower())+ \" | \" + lemmatize_sentences(sense_data.get('examples').lower())\n",
        "            dictionary_examples += getLemmaExamplesFromSenseDict(context_data[1], sense_data)\n",
        "    return dictionary_examples\n",
        "\n",
        "\n",
        "def getContextDictModelTwo(target_data, pos_data, corpus=False):\n",
        "    context_sense = []\n",
        "    target_sense = []\n",
        "    sentence = pos_data\n",
        "    sentence_length = len(sentence)\n",
        "    target_word = target_data.split(\".\")[0]\n",
        "    target_pos = target_data.split(\".\")[1]\n",
        "    for k in range(len(sentence)):\n",
        "        if sentence[k][0] == \"%%\":\n",
        "            target_index = k-1\n",
        "            targetWord = sentence[target_index][0]\n",
        "            break\n",
        "    \n",
        "    i = target_index-2\n",
        "    j = target_index+2\n",
        "    k = 0\n",
        "    while((i>=0 or j<len(sentence)) and k<30):   \n",
        "        if(i>=0 and len(sentence[i][2].strip())>= 3 and sentence[i][2].strip() != target_word):\n",
        "            context_word = sentence[i][2].strip()\n",
        "            context_pos = sentence[i][1].strip()\n",
        "            if(corpus):\n",
        "                sense = getNewSenses(context_word,context_pos)\n",
        "            else:\n",
        "                sense = getSenses(context_word,context_pos)\n",
        "            \n",
        "            if len(sense) >= 1:\n",
        "                context_sense.append([targetWord,context_word+\".\"+context_pos,sense, target_index-i])\n",
        "            \n",
        "        if(j<len(sentence) and len(sentence[j][2].strip())>= 3 and sentence[j][2].strip() != target_word):\n",
        "            context_word = sentence[j][2].strip()\n",
        "            context_pos = sentence[j][1].strip()\n",
        "            if(corpus):\n",
        "                sense = getNewSenses(context_word,context_pos)\n",
        "            else:\n",
        "                sense = getSenses(context_word,context_pos)\n",
        "                \n",
        "            if len(sense) >= 1:\n",
        "                context_sense.append([target_word,context_word+\".\"+context_pos,sense, j-target_index])\n",
        "            \n",
        "        i = i-1\n",
        "        j = j+1\n",
        "        k = k+1\n",
        "                \n",
        "    return context_sense\n",
        "\n",
        "\n",
        "def calculateSenseIdModelTwo(target_word_pos, pos_without_stopwords):\n",
        "    print(target_word_pos)\n",
        "    target_word_details = target_word_pos.split(\".\")\n",
        "    target_senses = getSenses(target_word_details[0].strip(), target_word_details[1].strip())\n",
        "    score_map = {}\n",
        "    context_sentence = limitizeContextMapModelTwo(getContextDictModelTwo(target_word_pos, pos_without_stopwords))\n",
        "    \n",
        "    for sense in target_senses:\n",
        "        #sense_examples = lemmatize_sentences(sense.get('gloss').lower())+ \" | \" + lemmatize_sentences(sense.get('examples').lower())\n",
        "        sense_examples = getLemmaExamplesFromSenseDict(target_word_pos.strip(), sense)\n",
        "        sense_example_words = sense_examples.split()\n",
        "        context_example_words = context_sentence.split()\n",
        "\n",
        "        common = set(sense_example_words).intersection( set(context_example_words) )\n",
        "        context_score = len(common)\n",
        "        score_map[sense.get('id')] = context_score\n",
        "    \n",
        "    key_max = max(score_map, key=score_map.get)\n",
        "    return key_max"
      ],
      "metadata": {
        "id": "gt-k61_UKbu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the new augemented dictionary by adding training data for corpus lesk\n",
        "def newDictionary():\n",
        "    parser = objectify.makeparser(recover=True)\n",
        "    tree = objectify.fromstring(''.join(open('dictionary.xml').readlines()), parser)\n",
        "    train_data_new = rename_columns(train_data)\n",
        "    for index, row in train_data_new.iterrows():\n",
        "        target_word = row['Target_Word'].strip()\n",
        "        sense_id = str(row['Sense_ID'])\n",
        "        sentence_to_add = row['Sentence']\n",
        "        \n",
        "        item = tree.xpath(\"//lexelt[@item='%s']\" % (target_word))\n",
        "        \n",
        "        for item_sense in item[0].getchildren():\n",
        "            if (str(item_sense.attrib['id']) == sense_id):\n",
        "                item_sense.attrib['examples'] = item_sense.attrib['examples'] + sentence_to_add\n",
        "\n",
        "    xml_new = etree.tostring(tree, pretty_print=True)\n",
        "    # save your xml\n",
        "    with open(r\"new_dictionary.xml\", \"wb\") as f:\n",
        "        f.write(xml_new)\n",
        "\n"
      ],
      "metadata": {
        "id": "iXv8J9JK3PLH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getLemmaExamplesFromCorpusSenseDict(word_pos, sense):\n",
        "    word_pos = word_pos.strip()\n",
        "    lemmaSenseKey = word_pos+ \"_\"+sense.get('id')\n",
        "    sense_examples = \"\"\n",
        "    if (lemmaSenseKey in SenseLemmaCorpusDictionary):\n",
        "        sense_examples = SenseLemmaCorpusDictionary.get(lemmaSenseKey)\n",
        "    else:\n",
        "        sense_examples = (\n",
        "            lemmatize_sentences(sense.get('gloss').lower())\n",
        "            + \" | \"\n",
        "            + ('.'.join(lemmatize_sentences(sentence.lower()) for sentence in sense.get('examples').split(\".\")))\n",
        "        )\n",
        "        SenseLemmaCorpusDictionary[lemmaSenseKey] = sense_examples\n",
        "    return sense_examples"
      ],
      "metadata": {
        "id": "xuHQqzJw4ssz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get sense from new dictionary\n",
        "## TOBE: pass the Tree as the paramter and combine into 1 function\n",
        "def getNewSenses(word, pos):\n",
        "    #global TreeNew\n",
        "    item = TreeNew.xpath(\"//lexelt[@item='%s.%s']\" % (word, pos))    \n",
        "    senses = []\n",
        "    if len(item) >= 1:\n",
        "        for sense in item[0].getchildren():\n",
        "            senses.append(dict(zip(sense.keys(), sense.values())))\n",
        "    return senses"
      ],
      "metadata": {
        "id": "d8MrUyCI9YOm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    global Tree\n",
        "    global TreeNew\n",
        "    global SenseLemmaDictionary\n",
        "    global SenseLemmaCorpusDictionary\n",
        "    SenseLemmaDictionary = {}\n",
        "    SenseLemmaCorpusDictionary = {}\n",
        "    # Read the dictionary file - original \n",
        "    Parser = objectify.makeparser(recover=True)\n",
        "    Tree = objectify.fromstring(''.join(open('dictionary.xml').readlines()), Parser)\n",
        "    \n",
        "    #read test data\n",
        "    train_data = pd.read_csv (r'train.data',header=None,delimiter = \"|\")\n",
        "    test_data = pd.read_csv (r'test.data',header=None,delimiter = \"|\")\n",
        "    validation_data = pd.read_csv (r'validate.data',header=None,delimiter = \"|\")\n",
        "\n",
        "    #rename columns for all the datasets\n",
        "    train_data_new = rename_columns(train_data)\n",
        "    test_data_new = rename_columns(test_data)\n",
        "    validation_data_new = rename_columns(validation_data)\n",
        "    \n",
        "    #create new dictionary\n",
        "    #newDictionary()\n",
        "    #ParserNew = objectify.makeparser(recover=True)\n",
        "    #TreeNew = objectify.fromstring(''.join(open('new_dictionary.xml').readlines()), ParserNew)\n",
        "    \n",
        "    ################################# Validation data ###################################\n",
        "    # validation set cleaning process\n",
        "    method_one_validation_df = validation_data_new\n",
        "    method_one_validation_df = lowercase_cleaned_data(method_one_validation_df, 'Sentence')\n",
        "    method_one_validation_df[\"pos_data\"] = method_one_validation_df['lowercase_cleaned'].swifter.apply(lambda sentence: retreive_pos_wordnet(sentence))\n",
        "    method_one_validation_df[\"pos_data\"] = method_one_validation_df[\"pos_data\"].swifter.apply(lambda pos_data_list: remove_stop_words_from_pos(pos_data_list))\n",
        "    \n",
        "    # Model 1 - Simple lesk\n",
        "    method_one_validation_df['simple_lesk_sense_id'] = method_one_validation_df.swifter.apply(lambda x: calculate_sense_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
        "    \n",
        "    \"\"\"\n",
        "    # Model 2 - Original Lesk\n",
        "    method_two_validation_df = method_one_validation_df\n",
        "    method_two_validation_df['original_lesk_sense_id'] = method_two_validation_df.swifter.apply(lambda x: calculateSenseIdModelTwo(x['Target_Word'], x['pos_data']), axis=1)\n",
        "    \n",
        "    # Model 3 - Advance original lesk\n",
        "    method_three_validation_df = method_two_validation_df\n",
        "    method_three_validation_df['adv_original_lesk_sense_id'] = method_three_validation_df.swifter.apply(lambda x: calculateSenseIdModel3(x['Target_Word'], x['pos_data']), axis=1)\n",
        "    \n",
        "    # Model 4 - Corpus lesk\n",
        "    method_four_validation_df = method_three_validation_df\n",
        "    method_four_validation_df['corpus_lesk_sense_id'] = method_four_validation_df.swifter.apply(lambda x: calculate_sense_corpus_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
        "    \n",
        "    # Model 5 - Adv Corpus lesk\n",
        "    method_five_validation_df = method_four_validation_df\n",
        "    method_five_validation_df['adv_corpus_lesk_sense_id'] = method_five_validation_df.swifter.apply(lambda x: calculateSenseIdModel3(x['Target_Word'], x['pos_data'], True), axis=1)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Accuracy of validation data for simple_lesk: \" + str(calculate_accuracy(method_one_validation_df, \"simple_lesk_sense_id\")))\n",
        "    \"\"\"\n",
        "    print(\"Accuracy of validation data for original_lesk: \" + str(calculate_accuracy(method_two_validation_df, \"original_lesk_sense_id\")))\n",
        "    print(\"Accuracy of validation data for adv_original_lesk: \" + str(calculate_accuracy(method_three_validation_df, \"adv_original_lesk_sense_id\")))\n",
        "    print(\"Accuracy of validation data for corpus_lesk: \" + str(calculate_accuracy(method_four_validation_df, \"corpus_lesk_sense_id\")))\n",
        "    print(\"Accuracy of validation data for adv_corpus_lesk: \" + str(calculate_accuracy(method_five_validation_df, \"adv_corpus_lesk_sense_id\")))\n",
        "    \"\"\"\n",
        "\n",
        "    # Export validation results to CSV\n",
        "    exportToCSV(method_one_validation_df, r'validation_results_SimpleLesk.csv')\n",
        "    "
      ],
      "metadata": {
        "id": "_PbPFE777sPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    ################################### Test data ######################################\n",
        "    \n",
        "    # test set cleaning process\n",
        "    method_one_test_df = test_data_new\n",
        "    method_one_test_df = lowercase_cleaned_data(method_one_test_df, 'Sentence')\n",
        "    method_one_test_df[\"pos_data\"] = method_one_test_df['lowercase_cleaned'].swifter.apply(lambda sentence: retreive_pos_wordnet(sentence))\n",
        "    method_one_test_df[\"pos_data\"] = method_one_test_df[\"pos_data\"].swifter.apply(lambda pos_data_list: remove_stop_words_from_pos(pos_data_list))\n",
        "    \n",
        "    # Model 1 - Simple Lesk\n",
        "    method_one_test_df['simple_lesk_sense_id'] = method_one_test_df.swifter.apply(lambda x: calculate_sense_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
        "    \n",
        "\n",
        "    # Export validation results to CSV\n",
        "    exportToCSV(method_one_test_df, r'test_data_results_SimpleLesk.csv')\n",
        "    "
      ],
      "metadata": {
        "id": "7mEBgEchDMh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    ################################### Training data ######################################\n",
        "    \n",
        "    # train data cleaning process\n",
        "    method_one_train_df = train_data_new\n",
        "    method_one_train_df = lowercase_cleaned_data(method_one_train_df, 'Sentence')\n",
        "    method_one_train_df[\"pos_data\"] = method_one_train_df['lowercase_cleaned'].swifter.apply(lambda sentence: retreive_pos_wordnet(sentence))\n",
        "    method_one_train_df[\"pos_data\"] = method_one_train_df[\"pos_data\"].swifter.apply(lambda pos_data_list: remove_stop_words_from_pos(pos_data_list))\n",
        "    \n",
        "    # Model 1 - Simple Lesk\n",
        "    method_one_train_df['simple_lesk_sense_id'] = method_one_train_df.swifter.apply(lambda x: calculate_sense_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
        "    \n",
        "     # Calculating accuracies of training data\n",
        "    print(\"Accuracy of training data for simple_lesk: \" + str(calculate_accuracy(method_one_train_df, \"simple_lesk_sense_id\")))\n",
        "\n",
        "     # Export validation results to CSV\n",
        "    exportToCSV(method_one_train_df, r'training_data_results.csv')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d0e9b954427945b6b6e7113b6da8ed4a",
            "77d623e5263d48adb53afe763855f354",
            "2412b9bac6d0483f983fd990192e713f",
            "e9a17fa718904828a0dfaea70e53df02",
            "58b1d638482844b0a8e56100977b34b3",
            "f78ba750e4e44d699be540feb9c884c5",
            "c7777544c93c47589275e232aaac8398",
            "a976213270304e79a6b9cd68d76a4f0d",
            "11a4c97f35664a2cbe38105602e190f8",
            "5c1d294bd6bf4b76a7cf5e2183b5cac7",
            "349ac179825a419c88166bee84053bc9"
          ]
        },
        "id": "fnX3qWCaDhFA",
        "outputId": "53602548-0990-4554-a242-72549b8e0337"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0e9b954427945b6b6e7113b6da8ed4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pandas Apply:   0%|          | 0/22281 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ]
}